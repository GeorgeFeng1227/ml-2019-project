{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "Reading files that contain the biased (WikiDetox) and unbiased (50/50 aggressive/unaggressive) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbias data inputed\n",
      "comment_data & target_unbias\n"
     ]
    }
   ],
   "source": [
    "#unbiased data input\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as nd\n",
    "import re\n",
    "\n",
    "Number_of_entry = 29560 #maximum acceptable 29560\n",
    "#input data and clean up\n",
    "agg_data = pd.read_csv('Aggressive_comments.tsv', sep = '\\t', nrows = Number_of_entry/2)\n",
    "neu_data = pd.read_csv('Neutral_comments.tsv', sep = '\\t', nrows = Number_of_entry/2)\n",
    "\n",
    "agg_comment = agg_data.comment\n",
    "neu_comment = neu_data.comment\n",
    "agg_target = agg_data.aggression\n",
    "neu_target = neu_data.aggression\n",
    "\n",
    "comment_data = np.asarray(agg_comment.append(neu_comment))\n",
    "target_unbias = np.asarray(agg_target.append(neu_target))\n",
    "print(\"unbias data inputed\")\n",
    "print(\"comment_data & target_unbias\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial data inputed\n",
      "trial_comment & trial_score\n"
     ]
    }
   ],
   "source": [
    "#Test unbiased scored data \n",
    "#new\n",
    "agg_data_s = pd.read_csv('Aggressive_scored.tsv', sep = '\\t', nrows = Number_of_entry/2)\n",
    "neu_data_s = pd.read_csv('Friendlier_scored.tsv', sep = '\\t', nrows = Number_of_entry/2)\n",
    "\n",
    "agg_comment_s = agg_data_s.comment\n",
    "neu_comment_s = neu_data_s.comment\n",
    "agg_target_s = agg_data_s.aggression_score\n",
    "neu_target_s = neu_data_s.aggression_score\n",
    "\n",
    "trial_comment = np.asarray(agg_comment_s.append(neu_comment_s))\n",
    "trial_score = np.asarray(agg_target_s.append(neu_target_s))\n",
    "print(\"trial data inputed\")\n",
    "print(\"trial_comment & trial_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Removes punctuation, newlinetoken, stop words for unbiased and biased data, lemmatizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step of preprocessing\n",
    "PUNCTUATION_NO_SPACE = re.compile(\"[.;:!*=<>`_'?Â¿,\\\"()\\[\\]]\")\n",
    "PUNCTUATION_SPACE = re.compile(\"-\")\n",
    "NEWLINE = re.compile(\"newlinetoken\")\n",
    "#This is more or less the nltk stop list with negations removed\n",
    "skip = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", 'im',\"youre\",\n",
    "        \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "        'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "        'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "        \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \n",
    "        'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'ive',\n",
    "        'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', \n",
    "        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "        'out', 'on', 'off', 'over', 'under', 'further', 'then', 'once', 'here', 'there', 'theres','when', 'where',\n",
    "        'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "        'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "        'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y','u','ur']\n",
    "\n",
    "#unbiased\n",
    "for i in range(comment_data.size):\n",
    "    comment_data[i] = PUNCTUATION_NO_SPACE.sub(\"\",comment_data[i].lower())\n",
    "    comment_data[i] = NEWLINE.sub(\"\",comment_data[i])\n",
    "    comment_data[i] = PUNCTUATION_SPACE.sub(\" \",comment_data[i].lower())\n",
    "    comment_data[i] = comment_data[i].split()\n",
    "    comment_data[i] = [word for word in comment_data[i] if word not in skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Kiara2.0/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing different ending of the same word\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    comment_data[i] = ' '.join([lemmatizer.lemmatize(word) for word in comment_data[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(target_unbias.size):\n",
    "    if (target_unbias[i] > 0.5):\n",
    "        target_unbias[i] = 1\n",
    "    else:\n",
    "        target_unbias[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for binary prediction\n",
    "Uses the TfidfVectorizer and LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbias data Accuracy:  0.8711163734776727\n"
     ]
    }
   ],
   "source": [
    "#model for binary predicting\n",
    "#default TF-idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "#print(type(comment_data[0]))\n",
    "ngram_arc = np.zeros(5)\n",
    "\n",
    "comment_data = np.asarray(comment_data)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vectorizer.fit(comment_data)\n",
    "comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "    \n",
    "def accuracy_test(ind, c=1, trials=50):\n",
    "    total = 0\n",
    "    #print(comment_vectorized.shape)\n",
    "    model = None\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, target_unbias, test_size = 0.1)\n",
    "\n",
    "        model = LogisticRegression(C=c)\n",
    "        model.fit(data_train, target_train)\n",
    "        total += accuracy_score(target_test, model.predict(data_test))\n",
    "    accuracy = total / trials\n",
    "    ngram_arc[ind] = accuracy\n",
    "    print(\"Unbias data Accuracy: \", accuracy)\n",
    "    return model\n",
    "\n",
    "model_ub = accuracy_test(ind = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interaction\n",
    "Uses LinearSVC model with Ngram = 1 to predict user input live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your comment here: insert random test comment here\n",
      "---Your input comment is classified as nonaggressive!---\n"
     ]
    }
   ],
   "source": [
    "#Predicting user input\n",
    "\n",
    "user_input = input(\"Write your comment here: \")\n",
    "#print(user_input)\n",
    "\n",
    "user_input = PUNCTUATION_NO_SPACE.sub(\"\",user_input.lower())\n",
    "user_input = NEWLINE.sub(\"\",user_input)\n",
    "user_input = PUNCTUATION_SPACE.sub(\" \",user_input.lower())\n",
    "user_input = user_input.split()\n",
    "user_input = [word for word in user_input if word not in skip]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "comment_data[i] = ' '.join([lemmatizer.lemmatize(word) for word in user_input])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vectorizer.fit(comment_data)\n",
    "user_input_vec = tfidf_vectorizer.transform(user_input)\n",
    "\n",
    "predicted_result = model_ub.predict(user_input_vec)\n",
    "\n",
    "if (np.mean(predicted_result > 0.5)):\n",
    "    print(\"---Your input comment is classified as aggressive!---\")\n",
    "else:\n",
    "    print(\"---Your input comment is classified as nonaggressive!---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
