{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'creative', 'dictionary', 'definitions', 'terms', 'insurance', 'ensurance', 'properly', 'applied', 'destruction', 'dont', 'understand', 'fine', 'legitimate', 'criticism', 'ill', 'write', 'three', 'man', 'cell', 'bounty', 'hunter', 'easy', 'understand', 'ensured', 'insured', 'different', 'differ', 'assuredthe', 'sentence', 'quote', 'absolutely', 'neutral', 'arent', 'familiar', 'underlying', 'theory', 'strike', 'back', 'eg', 'submarines', 'employed', 'nuclear', 'warfare', 'guiding', 'insurance', 'nor', 'likely', 'three', 'man', 'cell', 'structure', 'kept', 'ira', 'broken', 'british', 'thats', 'fault', 'fine', 'fix', 'explain', 'but', 'nothing', 'personal', 'creative', 'itim', 'tired', 'arguing', 'article', 'multi', 'party', 'turns', 'plenty', 'use', 'mutually', 'mutual', 'apply', 'standard', 'id', 'moving', 'mutual', 'assured', 'destruction', 'talk', 'not', 'appealing', 'reagan', 'voters', 'biases', 'effectiveness', 'dropping', 'lythere', 'double', 'standard', 'edits', 'comes', 'us', 'history', 'book', 'like', 'peace', 'movement', 'mad', 'defined', '1950', 'like', 'even', 'definition', 'totally', 'useless', '2002', 'historical', 'interest', 'makes', 'even', 'obvious', 'connection', 'implication', 'language', 'chosen', 'multiple', 'profession', 'specific', 'terms', 'consider', 'somehow', 'non', 'neutral', 'gandhi', 'thinks', 'eye', 'eye', 'describes', 'riots', 'death', 'penalty', 'war', 'but', 'dont', 'know', 'gandhi', 'doesntguess', 'reality', 'not', 'neutral', 'current', 'use', 'terms', 'slightly', 'controversial', 'neutrality', 'requires', 'negotiation', 'willingness', 'learnthis', 'problem', 'not', 'mine', 'may', 'dislike', 'writing', 'fine', 'fixed', 'but', 'disregarding', 'fundamental', 'axioms', 'philosphy', 'names', 'recur', 'multiple', 'phrases', 'failing', 'make', 'critical', 'distinctions', 'like', 'insurance', 'versus', 'assurance', 'versus', 'ensurance', 'made', 'one', 'quote', 'air', 'force', 'general', 'context', 'quote', 'disservice', 'readerif', 'someone', 'comes', 'research', 'topic', 'like', 'mad', 'want', 'context', 'beyond', 'historyif', 'history', 'book', 'fine', 'history', 'book', 'but', 'wasnt', 'claimed']\n"
     ]
    }
   ],
   "source": [
    "#Version 0.0 ---comment preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as nd\n",
    "import re\n",
    "\n",
    "Number_of_entry = 5000\n",
    "#input data and clean up\n",
    "data = pd.read_csv('aggression_annotated_comments.tsv', sep = '\\t', nrows = Number_of_entry)\n",
    "\n",
    "comment_data = data.comment\n",
    "comment_data = np.asarray(comment_data)\n",
    "\n",
    "PUNCTUATION_NO_SPACE = re.compile(\"[.;:!*`_'?,\\\"()\\[\\]]\")\n",
    "PUNCTUATION_SPACE = re.compile(\"-\")\n",
    "NEWLINE = re.compile(\"newlinetoken\")\n",
    "#This is more or less the nltk stop list with negations removed\n",
    "skip = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", 'im',\"youre\",\n",
    "        \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "        'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "        'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "        \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \n",
    "        'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'ive',\n",
    "        'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', \n",
    "        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "        'out', 'on', 'off', 'over', 'under', 'further', 'then', 'once', 'here', 'there', 'theres','when', 'where',\n",
    "        'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "        'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "        'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y','u','ur']\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    comment_data[i] = PUNCTUATION_NO_SPACE.sub(\"\",comment_data[i].lower())\n",
    "    comment_data[i] = NEWLINE.sub(\"\",comment_data[i])\n",
    "    comment_data[i] = PUNCTUATION_SPACE.sub(\" \",comment_data[i].lower())\n",
    "    comment_data[i] = comment_data[i].split()\n",
    "    comment_data[i] = [word for word in comment_data[i] if word not in skip]\n",
    "    \n",
    "print(comment_data[0])\n",
    "\n",
    "#stop_list = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Kiara2.0/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed followingall name early polish ruler ficticious therefore index naming oda von haldensleben husband dagome record first time ruler polanen tribe therefore indicated first document later developing land named polandthis quite comment name fictitious deserves least backing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    comment_data[i] = ' '.join([lemmatizer.lemmatize(word) for word in comment_data[i]])\n",
    "    \n",
    "print(comment_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "103624\n",
      "103624\n"
     ]
    }
   ],
   "source": [
    "annotation = pd.read_csv('aggression_annotations.tsv', sep = '\\t', nrows =Number_of_entry*10)\n",
    "target_raw = annotation.aggression\n",
    "target_pro = np.zeros(Number_of_entry)\n",
    "\n",
    "ctr = 0\n",
    "for i in range(target_pro.size):\n",
    "    aggr = 0\n",
    "    for j in range(10):\n",
    "        aggr += target_raw[ctr]\n",
    "        ctr += 1\n",
    "    if (aggr/10 > 0.5):\n",
    "        target_pro[i] = 1\n",
    "    else:\n",
    "        target_pro[i] = 0\n",
    "    \n",
    "print(target_pro[6])\n",
    "\n",
    "print(data.rev_id[6])\n",
    "print(annotation.rev_id[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#print(type(comment_data[0]))\n",
    "\n",
    "comment_data = np.asarray(comment_data)\n",
    "\n",
    "\"\"\"def accuracy_test(c=0.01, ngram=3, trials=100):\n",
    "    total = 0\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    tfidf_vectorizer.fit(comment_data)\n",
    "    comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "    #print(comment_vectorized.shape)\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "        model = LinearSVC(C=0.01)\n",
    "        model.fit(data_train, target_train)\n",
    "        total += accuracy_score(target_test, model.predict(data_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "accuracy_test()\n",
    "\"\"\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "tfidf_vectorizer.fit(comment_data)\n",
    "comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "model = LinearSVC(C=0.01)\n",
    "model.fit(data_train, target_train)\n",
    "accuracy = accuracy_score(target_test, model.predict(data_test))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friendly:  ('welcome', 0.044892251957242175)\n",
      "Friendly:  ('check one', 0.03023126133549706)\n",
      "Friendly:  ('fuck family', 0.027455303211724336)\n",
      "Friendly:  ('page', 0.027265067783274997)\n",
      "Friendly:  ('line', 0.02647619226522153)\n",
      "Aggressive:  ('but', -0.02304388471958091)\n",
      "Aggressive:  ('list', -0.01991580547643531)\n",
      "Aggressive:  ('article', -0.019876091397173695)\n",
      "Aggressive:  ('support', -0.01831803756604175)\n",
      "Aggressive:  ('attack', -0.017870539244158067)\n"
     ]
    }
   ],
   "source": [
    "#Words most associated with aggression/friendliness\n",
    "\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        tfidf_vectorizer.get_feature_names(), model.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "for best_positive in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print (\"Friendly: \", best_positive)\n",
    "    \n",
    "for best_negative in sorted(feature_to_coef.items(), key=lambda x: x[1])[:5]:\n",
    "    print (\"Aggressive: \", best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
