{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['removed', 'followingall', 'names', 'early', 'polish', 'rulers', 'ficticious', 'therefore', 'index', 'naming', 'oda', 'von', 'haldensleben', 'husband', 'dagome', 'records', 'first', 'time', 'rulers', 'polanen', 'tribe', 'therefore', 'indicated', 'first', 'document', 'later', 'developing', 'land', 'named', 'polandthis', 'quite', 'comment', 'names', 'fictitious', 'deserves', 'least', 'backing']\n"
     ]
    }
   ],
   "source": [
    "#Version 0.0 ---comment preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as nd\n",
    "import re\n",
    "\n",
    "Number_of_entry = 5000\n",
    "#input data and clean up\n",
    "data = pd.read_csv('aggression_annotated_comments.tsv', sep = '\\t', nrows = Number_of_entry)\n",
    "\n",
    "comment_data = data.comment\n",
    "comment_data = np.asarray(comment_data)\n",
    "\n",
    "PUNCTUATION_NO_SPACE = re.compile(\"[.;:!*`_'?,\\\"()\\[\\]]\")\n",
    "PUNCTUATION_SPACE = re.compile(\"-\")\n",
    "NEWLINE = re.compile(\"newlinetoken\")\n",
    "#This is more or less the nltk stop list with negations removed\n",
    "skip = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \\\n",
    "        \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \\\n",
    "        'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \\\n",
    "        'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \\\n",
    "        \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \\\n",
    "        'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', \\\n",
    "        'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', \\\n",
    "        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \\\n",
    "        'out', 'on', 'off', 'over', 'under', 'further', 'then', 'once', 'here', 'there', 'theres','when', 'where',\\\n",
    "        'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\\\n",
    "        'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \\\n",
    "        'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y','u','ur']\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    comment_data[i] = PUNCTUATION_NO_SPACE.sub(\"\",comment_data[i].lower())\n",
    "    comment_data[i] = NEWLINE.sub(\"\",comment_data[i])\n",
    "    comment_data[i] = PUNCTUATION_SPACE.sub(\" \",comment_data[i].lower())\n",
    "    comment_data[i] = comment_data[i].split()\n",
    "    comment_data[i] = [word for word in comment_data[i] if word not in skip]\n",
    "    \n",
    "print(comment_data[6])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stop_list = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opinion poll conducted mac 2000 45% consider taiwanese four year later think wonder mac either stopped hided opinion poll result would not stimulative prc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Fenggq/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    comment_data[i] = ' '.join([lemmatizer.lemmatize(word) for word in comment_data[i]])\n",
    "    \n",
    "print(comment_data[499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "annotation = pd.read_csv('aggression_annotations.tsv', sep = '\\t', nrows =Number_of_entry*10)\n",
    "target_raw = annotation.aggression\n",
    "target_pro = np.zeros(Number_of_entry)\n",
    "\n",
    "ctr = 0\n",
    "for i in range(target_pro.size):\n",
    "    aggr = 0\n",
    "    for j in range(10):\n",
    "        aggr += target_raw[ctr]\n",
    "        ctr += 1\n",
    "    if (aggr/10 > 0.5):\n",
    "        target_pro[i] = 1\n",
    "    else:\n",
    "        target_pro[i] = 0\n",
    "    \n",
    "print(target_pro[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "(5000, 27730)\n",
      "Accuracy:  0.914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "print(type(comment_data[0]))\n",
    "\n",
    "comment_data = np.asarray(comment_data)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(comment_data)\n",
    "comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "print(comment_vectorized.shape)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "model = LinearSVC(C=0.01)\n",
    "model.fit(data_train, target_train)\n",
    "accuracy = accuracy_score(target_test, model.predict(data_test))\n",
    "print(\"Accuracy: \", accuracy)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
