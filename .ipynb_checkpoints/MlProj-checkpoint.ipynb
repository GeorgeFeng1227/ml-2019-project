{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'creative', 'dictionary', 'definitions', 'terms', 'insurance', 'ensurance', 'properly', 'applied', 'destruction', 'dont', 'understand', 'fine', 'legitimate', 'criticism', 'ill', 'write', 'three', 'man', 'cell', 'bounty', 'hunter', 'easy', 'understand', 'ensured', 'insured', 'different', 'differ', 'assuredthe', 'sentence', 'quote', 'absolutely', 'neutral', 'arent', 'familiar', 'underlying', 'theory', 'strike', 'back', 'eg', 'submarines', 'employed', 'nuclear', 'warfare', 'guiding', 'insurance', 'nor', 'likely', 'three', 'man', 'cell', 'structure', 'kept', 'ira', 'broken', 'british', 'thats', 'fault', 'fine', 'fix', 'explain', 'but', 'nothing', 'personal', 'creative', 'itim', 'tired', 'arguing', 'article', 'multi', 'party', 'turns', 'plenty', 'use', 'mutually', 'mutual', 'apply', 'standard', 'id', 'moving', 'mutual', 'assured', 'destruction', 'talk', 'not', 'appealing', 'reagan', 'voters', 'biases', 'effectiveness', 'dropping', 'lythere', 'double', 'standard', 'edits', 'comes', 'us', 'history', 'book', 'like', 'peace', 'movement', 'mad', 'defined', '1950', 'like', 'even', 'definition', 'totally', 'useless', '2002', 'historical', 'interest', 'makes', 'even', 'obvious', 'connection', 'implication', 'language', 'chosen', 'multiple', 'profession', 'specific', 'terms', 'consider', 'somehow', 'non', 'neutral', 'gandhi', 'thinks', 'eye', 'eye', 'describes', 'riots', 'death', 'penalty', 'war', 'but', 'dont', 'know', 'gandhi', 'doesntguess', 'reality', 'not', 'neutral', 'current', 'use', 'terms', 'slightly', 'controversial', 'neutrality', 'requires', 'negotiation', 'willingness', 'learnthis', 'problem', 'not', 'mine', 'may', 'dislike', 'writing', 'fine', 'fixed', 'but', 'disregarding', 'fundamental', 'axioms', 'philosphy', 'names', 'recur', 'multiple', 'phrases', 'failing', 'make', 'critical', 'distinctions', 'like', 'insurance', 'versus', 'assurance', 'versus', 'ensurance', 'made', 'one', 'quote', 'air', 'force', 'general', 'context', 'quote', 'disservice', 'readerif', 'someone', 'comes', 'research', 'topic', 'like', 'mad', 'want', 'context', 'beyond', 'historyif', 'history', 'book', 'fine', 'history', 'book', 'but', 'wasnt', 'claimed']\n"
     ]
    }
   ],
   "source": [
    "#Version 0.0 ---comment preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as nd\n",
    "import re\n",
    "\n",
    "Number_of_entry = 5000\n",
    "#input data and clean up\n",
    "data = pd.read_csv('aggression_annotated_comments.tsv', sep = '\\t', nrows = Number_of_entry)\n",
    "\n",
    "comment_data = data.comment\n",
    "comment_data = np.asarray(comment_data)\n",
    "\n",
    "PUNCTUATION_NO_SPACE = re.compile(\"[.;:!*=<>`_'?,\\\"()\\[\\]]\")\n",
    "PUNCTUATION_SPACE = re.compile(\"-\")\n",
    "NEWLINE = re.compile(\"newlinetoken\")\n",
    "#This is more or less the nltk stop list with negations removed\n",
    "skip = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", 'im',\"youre\",\n",
    "        \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "        'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "        'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "        \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', \n",
    "        'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'ive',\n",
    "        'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', \n",
    "        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "        'out', 'on', 'off', 'over', 'under', 'further', 'then', 'once', 'here', 'there', 'theres','when', 'where',\n",
    "        'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "        'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "        'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y','u','ur']\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    comment_data[i] = PUNCTUATION_NO_SPACE.sub(\"\",comment_data[i].lower())\n",
    "    comment_data[i] = NEWLINE.sub(\"\",comment_data[i])\n",
    "    comment_data[i] = PUNCTUATION_SPACE.sub(\" \",comment_data[i].lower())\n",
    "    comment_data[i] = comment_data[i].split()\n",
    "    comment_data[i] = [word for word in comment_data[i] if word not in skip]\n",
    "    \n",
    "print(comment_data[0])\n",
    "\n",
    "#stop_list = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Kiara2.0/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed followingall name early polish ruler ficticious therefore index naming oda von haldensleben husband dagome record first time ruler polanen tribe therefore indicated first document later developing land named polandthis quite comment name fictitious deserves least backing\n"
     ]
    }
   ],
   "source": [
    "#Removing different ending of the same word\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for i in range(comment_data.size):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    comment_data[i] = ' '.join([lemmatizer.lemmatize(word) for word in comment_data[i]])\n",
    "    \n",
    "print(comment_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Number of Aggressive comments within the sample: 435\n",
      "Number of Neutral comments within the sample: 4565\n",
      "Percentage of Aggressive comments: % 8.7\n"
     ]
    }
   ],
   "source": [
    "#Input target for binary model(aggressive or neutral)\n",
    "annotation = pd.read_csv('aggression_annotations.tsv', sep = '\\t', nrows =Number_of_entry*20)\n",
    "rev_id = annotation.rev_id\n",
    "target_raw = annotation.aggression\n",
    "target_pro = np.zeros(Number_of_entry)\n",
    "\n",
    "ctr = 0\n",
    "ctr_aggre = 0\n",
    "ctr_neut = 0\n",
    "for i in range(target_pro.size):\n",
    "    ctr2 = 0\n",
    "    aggr = 0\n",
    "    curr_rev_id = rev_id[ctr]\n",
    "    while curr_rev_id == rev_id[ctr]:\n",
    "        aggr += target_raw[ctr]\n",
    "        ctr += 1\n",
    "        ctr2 += 1\n",
    "    #print(ctr2)\n",
    "    if (aggr/ctr2 > 0.5):\n",
    "        target_pro[i] = 1\n",
    "        ctr_aggre += 1\n",
    "    else:\n",
    "        target_pro[i] = 0\n",
    "        ctr_neut += 1\n",
    "    \n",
    "print(target_pro[1])\n",
    "print('Number of Aggressive comments within the sample:', ctr_aggre)\n",
    "print('Number of Neutral comments within the sample:', ctr_neut)\n",
    "print('Percentage of Aggressive comments: %', (ctr_aggre / Number_of_entry)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "89320\n"
     ]
    }
   ],
   "source": [
    "#Calculates target scores based on scores of 2 to -2 \n",
    "#(most friendly to most aggressive)\n",
    "score_raw = annotation.aggression_score\n",
    "score_pro = np.zeros(Number_of_entry)\n",
    "\n",
    "ctr = 0\n",
    "for i in range(score_pro.size):\n",
    "    count = 0\n",
    "    aggr = 0\n",
    "    curr_rev_id = rev_id[ctr]\n",
    "    while curr_rev_id == rev_id[ctr]:\n",
    "        aggr += score_raw[ctr]\n",
    "        count += 1\n",
    "        ctr += 1 \n",
    "    if (count == 0):\n",
    "        print(ctr)\n",
    "        print(rev_id[ctr])\n",
    "        break\n",
    "    if (aggr/count > 1.5):\n",
    "        score_pro[i] = 4\n",
    "    elif (aggr/count > 0.5):\n",
    "        score_pro[i] = 3\n",
    "    elif (aggr/count > -0.5):\n",
    "        score_pro[i] = 2\n",
    "    elif (aggr/count > -1.5):\n",
    "        score_pro[i] = 1\n",
    "    else:\n",
    "        score_pro[i] = 0\n",
    "    \n",
    "print(score_pro[3])\n",
    "print(rev_id[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9141200000000004\n",
      "0.906\n"
     ]
    }
   ],
   "source": [
    "#model for binary predicting\n",
    "#default TF-idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "#print(type(comment_data[0]))\n",
    "\n",
    "comment_data = np.asarray(comment_data)\n",
    "\n",
    "\n",
    "def accuracy_test(c=0.01, ngram=3, trials=100):\n",
    "    total = 0\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    tfidf_vectorizer.fit(comment_data)\n",
    "    comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "    #print(comment_vectorized.shape)\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "        model = LinearSVC(C=0.01)\n",
    "        model.fit(data_train, target_train)\n",
    "        total += accuracy_score(target_test, model.predict(data_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "#accuracy_test()\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "tfidf_vectorizer.fit(comment_data)\n",
    "comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "model = LinearSVC(C=0.01)\n",
    "model.fit(data_train, target_train)\n",
    "accuracy = accuracy_score(target_test, model.predict(data_test))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggressive:  ('fuck', 0.2559775954182716)\n",
      "Aggressive:  ('fucking', 0.12197724829680769)\n",
      "Aggressive:  ('suck', 0.10299288748170238)\n",
      "Aggressive:  ('shut', 0.10250002825796391)\n",
      "Aggressive:  ('bitch', 0.09437073708905781)\n",
      "Friendly:  ('article', -0.0961834931906497)\n",
      "Friendly:  ('would', -0.05961513258173129)\n",
      "Friendly:  ('but', -0.054063349048217864)\n",
      "Friendly:  ('one', -0.052003715185273275)\n",
      "Friendly:  ('page', -0.05055706840075406)\n"
     ]
    }
   ],
   "source": [
    "#Words most associated with aggression/friendliness\n",
    "\n",
    "def agg_friendly_words(model):\n",
    "    feature_to_coef = {\n",
    "        word: coef for word, coef in zip(\n",
    "            tfidf_vectorizer.get_feature_names(), model.coef_[0]\n",
    "        )\n",
    "    }\n",
    "\n",
    "    for best_negative in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print (\"Aggressive: \", best_negative)\n",
    "\n",
    "    for best_positive in sorted(feature_to_coef.items(), key=lambda x: x[1])[:5]:\n",
    "        print (\"Friendly: \", best_positive)\n",
    "\n",
    "agg_friendly_words(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kiara/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Kiara/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression 0.786\n"
     ]
    }
   ],
   "source": [
    "#Training and testing with score data\n",
    "data_s_train, data_s_test, score_train, score_test = train_test_split(\n",
    "    comment_vectorized, score_pro, test_size = 0.1)\n",
    "\n",
    "score_model = LinearSVC(C=0.01)\n",
    "score_model.fit(data_s_train, score_train)\n",
    "score_accuracy = accuracy_score(score_test, score_model.predict(data_s_test))\n",
    "print(\"Linear SVC\", score_accuracy)\n",
    "\n",
    "lr_model = LogisticRegression(C=1)\n",
    "lr_model.fit(data_s_train, score_train)\n",
    "lr_accuracy = accuracy_score(score_test, lr_model.predict(data_s_test))\n",
    "print(\"Logistic regression\", lr_accuracy)\n",
    "\n",
    "\n",
    "def accuracy_test(c=0.01, trials=100):\n",
    "    total = 0\n",
    "    for i in range(trials):\n",
    "        data_s_train, data_s_test, score_train, score_test = train_test_split(\n",
    "            comment_vectorized, score_pro, test_size = 0.1)\n",
    "\n",
    "        score_model = LinearSVC(C=0.01,multi_class='ovr')\n",
    "        score_model.fit(data_s_train, score_train)\n",
    "        total += accuracy_score(score_test, score_model.predict(data_s_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "#accuracy_test()\n",
    "\n",
    "def lr_test(c=0.01, trials=100):\n",
    "    total = 0\n",
    "    for i in range(trials):\n",
    "        data_s_train, data_s_test, score_train, score_test = train_test_split(\n",
    "            comment_vectorized, score_pro, test_size = 0.1)\n",
    "\n",
    "        score_model = LogisticRegression(C=c,multi_class='ovr')\n",
    "        score_model.fit(data_s_train, score_train)\n",
    "        total += accuracy_score(score_test, score_model.predict(data_s_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"LR accuracy: \", accuracy)\n",
    "#lr_test(c=0.05)\n",
    "#lr_test(c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggressive:  ('fuck', 0.2559775954182716)\n",
      "Aggressive:  ('fucking', 0.12197724829680769)\n",
      "Aggressive:  ('suck', 0.10299288748170238)\n",
      "Aggressive:  ('shut', 0.10250002825796391)\n",
      "Aggressive:  ('bitch', 0.09437073708905781)\n",
      "Friendly:  ('article', -0.0961834931906497)\n",
      "Friendly:  ('would', -0.05961513258173129)\n",
      "Friendly:  ('but', -0.054063349048217864)\n",
      "Friendly:  ('one', -0.052003715185273275)\n",
      "Friendly:  ('page', -0.05055706840075406)\n",
      "\n",
      "\n",
      "Aggressive:  ('fuck', 0.2573759188261473)\n",
      "Aggressive:  ('fucking', 0.09063946949338575)\n",
      "Aggressive:  ('suck', 0.08881899053163736)\n",
      "Aggressive:  ('fucker', 0.07900107472517616)\n",
      "Aggressive:  ('go fuck', 0.06289810043575718)\n",
      "Friendly:  ('article', -0.058339734032672834)\n",
      "Friendly:  ('not', -0.05574827759141416)\n",
      "Friendly:  ('page', -0.041523789875916886)\n",
      "Friendly:  ('would', -0.038854932726491966)\n",
      "Friendly:  ('but', -0.03234783114133053)\n",
      "\n",
      "\n",
      "Aggressive:  ('fuck', 5.122243572368046)\n",
      "Aggressive:  ('suck', 2.003587906452357)\n",
      "Aggressive:  ('fucking', 1.9840693778207188)\n",
      "Aggressive:  ('fucker', 1.706278201817971)\n",
      "Aggressive:  ('stupid', 1.3784618244035933)\n",
      "Friendly:  ('article', -1.0977043913212665)\n",
      "Friendly:  ('not', -0.9728504739966084)\n",
      "Friendly:  ('would', -0.7657024538322734)\n",
      "Friendly:  ('page', -0.7428097821684273)\n",
      "Friendly:  ('one', -0.5503022281173181)\n"
     ]
    }
   ],
   "source": [
    "agg_friendly_words(model)\n",
    "print(\"\\n\")\n",
    "agg_friendly_words(score_model)\n",
    "print(\"\\n\")\n",
    "agg_friendly_words(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for binary predicting\n",
    "# TF-idf vectorizer with 'char_wb' analyzer\n",
    "def accuracy_test_wb(c=0.01, ngram=3, trials=100):\n",
    "    total = 0\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb')\n",
    "    tfidf_vectorizer.fit(comment_data)\n",
    "    comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "    #print(comment_vectorized.shape)\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "        model = LinearSVC(C=0.01)\n",
    "        model.fit(data_train, target_train)\n",
    "        total += accuracy_score(target_test, model.predict(data_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    \n",
    "# TF-idf vectorizer with 'char' analyzer\n",
    "def accuracy_test_char(c=0.01, ngram=3, trials=100):\n",
    "    total = 0\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "    tfidf_vectorizer.fit(comment_data)\n",
    "    comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "    #print(comment_vectorized.shape)\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, target_pro, test_size = 0.1)\n",
    "\n",
    "        model = LinearSVC(C=0.01)\n",
    "        model.fit(data_train, target_train)\n",
    "        total += accuracy_score(target_test, model.predict(data_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "#accuracy_test_wb(ngram=5)\n",
    "#accuracy_test_char(ngram=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for score predicting\n",
    "# TF-idf vectorizer with 'char' analyzer\n",
    "def score_test_char(c=0.01, ngram=5, trials=100):\n",
    "    total = 0\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "    tfidf_vectorizer.fit(comment_data)\n",
    "    comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, score_pro, test_size = 0.1)\n",
    "\n",
    "        model = LinearSVC(C=0.01)\n",
    "        model.fit(data_s_train, score_train)\n",
    "        total += accuracy_score(score_test, model.predict(data_s_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# TF-idf vectorizer with 'char_wb' analyzer\n",
    "def score_test_wb(c=0.01, ngram=5, trials=100):\n",
    "    total = 0\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb')\n",
    "    tfidf_vectorizer.fit(comment_data)\n",
    "    comment_vectorized = tfidf_vectorizer.transform(comment_data)\n",
    "\n",
    "    for i in range(trials):\n",
    "        data_train, data_test, target_train, target_test = train_test_split(\n",
    "            comment_vectorized, score_pro, test_size = 0.1)\n",
    "\n",
    "        model = LinearSVC(C=0.01)\n",
    "        model.fit(data_s_train, score_train)\n",
    "        total += accuracy_score(score_test, model.predict(data_s_test))\n",
    "    accuracy = total / trials\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "#score_test_wb()\n",
    "#score_test_char()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(0,)\n",
      "(5000,)\n",
      "(0,)\n",
      "(0,)\n",
      "0.8024\n",
      "4012\n",
      "0.0\n",
      "0.0\n",
      "1.2462612163509472\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#print(comment_vectorized.shape[0])\n",
    "#print(data_test[0])\n",
    "#for i in range(data_train.shape[0]):\n",
    "\n",
    "pred_result = score_model.predict(comment_vectorized)\n",
    "#print(pred_result.shape)\n",
    "\n",
    "index_arr0 = []\n",
    "index_arr1 = []\n",
    "index_arr2 = []\n",
    "index_arr3 = []\n",
    "index_arr4 = []\n",
    "\n",
    "pred_ctr0 = 0\n",
    "pred_ctr1 = 0\n",
    "pred_ctr2 = 0\n",
    "pred_ctr3 = 0\n",
    "pred_ctr4 = 0\n",
    "\n",
    "for i in range(pred_result.size):\n",
    "    #print(int(pred_result[0]))\n",
    "    if (int(pred_result[i]) == 4):\n",
    "        index_arr4.append(i)\n",
    "        pred_ctr4 += 1\n",
    "    elif(int(pred_result[i]) == 3):\n",
    "        #print(i)\n",
    "        #print(comment_data[i])\n",
    "        index_arr3.append(i)\n",
    "        pred_ctr += 1\n",
    "    elif (int(pred_result[i]) == 2):\n",
    "        index_arr2.append(i)\n",
    "        pred_ctr2 += 1\n",
    "    elif (int(pred_result[i]) == 1):\n",
    "        index_arr1.append(i)\n",
    "        pred_ctr1 += 1\n",
    "    elif (int(pred_result[i]) == 0):\n",
    "        index_arr0.append(i)\n",
    "        pred_ctr0 += 1\n",
    "        \n",
    "index_arr4 = np.asarray(index_arr4)\n",
    "index_arr3 = np.asarray(index_arr3)\n",
    "index_arr2 = np.asarray(index_arr2)\n",
    "index_arr1 = np.asarray(index_arr1)\n",
    "index_arr0 = np.asarray(index_arr0)\n",
    "print(index_arr4.shape)\n",
    "print(index_arr3.shape)\n",
    "print(index_arr2.shape)\n",
    "print(index_arr1.shape)\n",
    "print(index_arr0.shape)\n",
    "\n",
    "\n",
    "accuracy2 = accuracy_score(score_pro, score_model.predict(comment_vectorized))\n",
    "print(accuracy2)\n",
    "\n",
    "num0 = 0\n",
    "num1 = 0\n",
    "num2 = 0\n",
    "num3 = 0\n",
    "num4 = 0\n",
    "for i in range(score_pro.size):\n",
    "    if (score_pro[i] == 0) and (i not in index_arr0):\n",
    "        num0 += 1\n",
    "    elif (score_pro[i] == 1) and (i not in index_arr1):\n",
    "        num1 += 1\n",
    "    elif score_pro[i] == 2:\n",
    "        num2 += 1\n",
    "    elif (score_pro[i] == 3) and (i not in index_arr3):\n",
    "        num3 += 1\n",
    "    elif (score_pro[i] == 4) and (i not in index_arr4):\n",
    "        num4 += 1\n",
    "print(num2)        \n",
    "print(pred_ctr0 / num0)\n",
    "print(pred_ctr1 / num1)\n",
    "print(pred_ctr2 / num2)\n",
    "print(pred_ctr3 / num3)\n",
    "print(pred_ctr4 / num4)\n",
    "\n",
    "#Essentially, 80% of the 5000 comments give an aggression score of 2\n",
    "#The model is thus predicting 2 for everything and getting an accuracy of 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
